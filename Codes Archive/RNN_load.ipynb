{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:pynput was not found; mouse and keyboard input will not be available.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from scamp import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Flatten,Softmax,Input\n",
    "import keras.backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 20000) dtype=int32, numpy=\n",
       "array([[15, 12, 23, ..., 10, 13,  9],\n",
       "       [ 0,  1,  0, ...,  1,  0,  1]])>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Species.csv', sep=\",\",index_col = 0)\n",
    "n = df.shape[0]\n",
    "X = tf.Variable(np.zeros((2,n*2)),dtype='int32')  # any data tensor\n",
    "for i in range(0,2*n,2):\n",
    "    X[0,i].assign(df['Cantus'][i/2] - 43)\n",
    "    X[1,i].assign(0)\n",
    "    X[0,i+1].assign(df['Counter'][i/2] - 55)\n",
    "    X[1,i+1].assign(1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20000, 31), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth = 30\n",
    "X = tf.concat([tf.one_hot(X[0,:], depth),tf.dtypes.cast(tf.reshape(X[1,:],[20000,1]),tf.float32)],1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_model():\n",
    "    \n",
    "    rnn_units = 12\n",
    "    n_notes = 30\n",
    "\n",
    "    input_A = keras.layers.Input(shape=(20,31), name=\"input_A\") # (15, 20, 31)\n",
    "    input_B = keras.layers.Input(shape=(1,31), name=\"input_B\") # (15, 1, 31)\n",
    "\n",
    "    hidden1 = keras.layers.LSTM(rnn_units, return_sequences=True)(input_A) # (15,20,12)\n",
    "    hidden2 = keras.layers.LSTM(rnn_units, return_sequences=True)(hidden1) #(15,20,12)\n",
    "    \n",
    "    output_RNN = keras.layers.Dense(n_notes, activation='softmax', name = 'output_RNN')(hidden2) #(15,20,30)\n",
    "\n",
    "    e = keras.layers.Dense(1, activation='tanh')(hidden2) #(15,20,1)\n",
    "    e = keras.layers.Reshape([-1])(e) #(15,20)\n",
    "\n",
    "    alpha = keras.layers.Activation('softmax')(e) #(15,20)\n",
    "    c = keras.layers.Permute([2, 1])(keras.layers.RepeatVector(rnn_units)(alpha)) #(15,20,12)\n",
    "    c = keras.layers.Multiply()([hidden2, c]) #(15,20,12)\n",
    "    c = keras.layers.Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c) #(15,12)\n",
    "\n",
    "    output_A = keras.layers.Dense(n_notes, activation = 'softmax', name = 'output_A')(c) #(15,30) (0, cantus)\n",
    "\n",
    "    aux = keras.layers.SimpleRNN(rnn_units)(input_B, initial_state=c) #(15,12)\n",
    "    output_B = keras.layers.Dense(n_notes, activation = 'softmax', name = 'output_B')(aux) #(15,30) (1, counter)\n",
    "\n",
    "    model = keras.models.Model([input_A, input_B], [output_RNN, output_A, output_B])\n",
    "    aux_model = keras.models.Model([input_A, input_B], [output_A, output_B])\n",
    "    sub_model = keras.models.Model(input_A, output_A)\n",
    "    att_model = keras.models.Model([input_A, input_B], alpha)\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy','categorical_crossentropy'],\n",
    "              loss_weights=[0.2, 0.4, 0.4],optimizer=\"sgd\")\n",
    "    aux_model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "              optimizer=\"sgd\")\n",
    "    sub_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"sgd\")\n",
    "    return model,aux_model,sub_model,att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_1_,aux_model_0_1_, sub_model_0_1_,att_model_0_1_ = creat_model()\n",
    "model_1_0_,aux_model_1_0_, sub_model_1_0_,att_model_1_0_ = creat_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_A (InputLayer)            [(None, 20, 31)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 20, 12)       2112        input_A[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 12)       1200        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20, 1)        13          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 20)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20)           0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 12, 20)       0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 20, 12)       0           repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 20, 12)       0           lstm_1[0][0]                     \n",
      "                                                                 permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 12)           0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_B (InputLayer)            [(None, 1, 31)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn (SimpleRNN)          (None, 12)           528         input_B[0][0]                    \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output_RNN (Dense)              (None, 20, 30)       390         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output_A (Dense)                (None, 30)           390         lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output_B (Dense)                (None, 30)           390         simple_rnn[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,023\n",
      "Trainable params: 5,023\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0_1_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_1_.load_weights('model_0_1_weight.h5')\n",
    "model_1_0_.load_weights('model_1_0_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(initial,seq_length):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    initial -- initial sequence of shape (1,20,31) (1 sample, sequence of length 20, vector fearure: one-hot(30);cantus(0),counter(1))\n",
    "    seq_length -- # pairs of notes generated\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    sequence = initial\n",
    "    pred_0_1_0 = []\n",
    "    pred_0_1_1 = []\n",
    "    pred_1_0_1 = []\n",
    "    pred_1_0_0 = []\n",
    "    \n",
    "    for i in range(seq_length):\n",
    "        output_A_0_1_ = sub_model_0_1_.predict(initial).reshape([30,])\n",
    "        pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_)[0]\n",
    "        pred_0_1_0.append(pred_A_class_random_0_1_+43)\n",
    "        \n",
    "        output_A_1_0_ = sub_model_1_0_.predict(initial).reshape([30,])\n",
    "        pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_)[0]\n",
    "        pred_1_0_1.append(pred_A_class_random_1_0_+55)\n",
    "\n",
    "        one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "        out_0_1_0 = tf.concat((one_hot,[[[0]]]),2)\n",
    "        \n",
    "        one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "        out_1_0_1 = tf.concat((one_hot,[[[1]]]),2)\n",
    "\n",
    "        output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "        output_B_0_1_ = y_pred_B_0_1_.reshape([30,])\n",
    "        pred_B_class_random_0_1_ = np.random.choice(30, 1, p=output_B_0_1_)[0]\n",
    "        pred_0_1_1.append(pred_B_class_random_0_1_+55)\n",
    "        \n",
    "        output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "        output_B_1_0_ = y_pred_B_1_0_.reshape([30,])\n",
    "        pred_B_class_random_1_0_ = np.random.choice(30, 1, p=output_B_1_0_)[0]\n",
    "        pred_1_0_0.append(pred_B_class_random_1_0_+43)\n",
    "\n",
    "        one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "        out_0_1_1 = tf.concat((one_hot,[[[1]]]),2)\n",
    "        \n",
    "        one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "        out_1_0_0 = tf.concat((one_hot,[[[0]]]),2)\n",
    "        \n",
    "        sequence = tf.concat((sequence,out_1_0_0),1)\n",
    "        sequence = tf.concat((sequence,out_0_1_1),1)\n",
    "\n",
    "        initial = sequence[:,-20:,:]\n",
    "        \n",
    "    return sequence,pred_0_1_0,pred_0_1_1,pred_1_0_1,pred_1_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40, 31), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_start_A = tf.reshape(X[9600:9620,:],[1,20,31])\n",
    "seq_length = 10\n",
    "sequence,pred_0_1_0,pred_0_1_1,pred_1_0_1,pred_1_0_0 = generate_sequence(X_start_A,seq_length)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tf.reshape(sequence,[20+2*seq_length,31])\n",
    "sequence = np.array(sequence)\n",
    "sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantus_lst = pred_1_0_0;\n",
    "counter_lst = pred_0_1_1;\n",
    "pred_cantus_lst = pred_0_1_0\n",
    "pred_counter_lst = pred_1_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57, 55, 62, 62, 59, 60, 65, 58, 52, 56]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cantus_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 61, 66, 58, 66, 64, 62, 56, 60, 51]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cantus_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 65, 69, 65, 73, 72, 66, 65, 68, 58]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 63, 70, 71, 62, 69, 69, 67, 61, 60]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_counter_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10,  7,  3, 14, 12,  1,  7, 16,  2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(counter_lst) - np.array(cantus_lst) # real harmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 4, 3, 7, 7, 8, 4, 9, 8, 7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(counter_lst) - np.array(pred_cantus_lst) # 0_1_harmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, 9, 3, 9, 4, 9, 9, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_counter_lst) - np.array(cantus_lst) # 1_0_harmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2,  7,  0, -3,  1,  5, -7, -6,  4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(cantus_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3,  4, -1,  0, -7,  0, -6,  6, -1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(pred_cantus_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4, -5,  0, -6, -1, -5,  6, -5])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(counter_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   9, -10,  -5,   1,  -3,   9,  -9,   0])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(pred_counter_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4,  1,  1, -7, -7, -4, -7,  8, -3, -1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cantus_lst) - np.array(pred_cantus_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   2,  -3,   2,   7,   0,   2, -12,   3,  -2])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(counter_lst) - np.array(pred_counter_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preset Piano Merlin for piano\n",
      "Using preset Piano Merlin for piano\n"
     ]
    }
   ],
   "source": [
    "s = Session()\n",
    "s.tempo = 180\n",
    "piano1 = s.new_part(\"piano\")\n",
    "piano2 = s.new_part(\"piano\")\n",
    "\n",
    "def cantus():\n",
    "    for i in cantus_lst:\n",
    "        piano2.play_note(i,1,4)\n",
    "        \n",
    "def counter():\n",
    "    for i in counter_lst:\n",
    "        piano1.play_note(i,1,4)\n",
    "        \n",
    "        \n",
    "s.fast_forward_to_beat(100)\n",
    "\n",
    "s.start_transcribing()\n",
    "s.fork(counter)\n",
    "s.fork(cantus)\n",
    "s.wait_for_children_to_finish()\n",
    "performance = s.stop_transcribing()\n",
    "performance.to_score(title = \"First Species Counterpoint\", composer = \"My programme\",time_signature = \"4/4\").show_xml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20000, 31), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[3, 4],\n",
       "       [5, 6],\n",
       "       [1, 2]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.shuffle([[1, 2],\n",
    " [3, 4], \n",
    " [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(X[:9600,:]) # !!!!\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(X[9600:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "window_length = n_steps\n",
    "dataset = train_dataset.window(window_length, shift=2, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "shuffle_dataset = dataset.shuffle(10000).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "(20, 20, 31)\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for window in shuffle_dataset:\n",
    "    i = i + 1\n",
    "    print(window.shape)\n",
    "    #print(window)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 998us/step - loss: 3.0221\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.5853\n",
      "1/1 [==============================] - 0s 988us/step - loss: 2.7912\n",
      "1/1 [==============================] - 0s 982us/step - loss: 3.3339\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7132\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8867\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.5042\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9348\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0003\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9053\n",
      "1/1 [==============================] - 0s 977us/step - loss: 2.6407\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9324\n"
     ]
    }
   ],
   "source": [
    "# one step forward\n",
    "# without communication\n",
    "# without dominate - subordinate relation\n",
    "\n",
    "counter = 0;\n",
    "for window in shuffle_dataset: #(18,20,31)\n",
    "    \n",
    "    output_A_0_1_ = sub_model_0_1_.predict(window) #(18,30)\n",
    "    out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "    for i in range(batch_size):\n",
    "        pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "        one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "        out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "    \n",
    "    output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((window,out_0_1_0))\n",
    "    y_pred_B_0_1_  #(18,30)\n",
    "    out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "    for i in range(batch_size):\n",
    "        pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "        one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "        out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "    \n",
    "    output_A_1_0_ = sub_model_1_0_.predict(window) #(18,30)\n",
    "    out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "    for i in range(batch_size):\n",
    "        pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "        one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "        out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "        \n",
    "    output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((window,out_1_0_1))\n",
    "    y_pred_B_1_0_  #(18,30)\n",
    "    out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "    for i in range(batch_size):\n",
    "        pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "        one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "        out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "        \n",
    "    target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "    target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "    target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "    target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "    \n",
    "    if counter%40 == 0:\n",
    "        sub_model_0_1_.fit(initial,target_1_0_0,verbose = 1)\n",
    "        sub_model_1_0_.fit(initial,target_0_1_1,verbose = 1)\n",
    "    else:\n",
    "        sub_model_0_1_.fit(initial,target_1_0_0,verbose = 0)\n",
    "        sub_model_1_0_.fit(initial,target_0_1_1,verbose = 0)\n",
    "        \n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 998us/step - loss: 2.6700\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7504\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.8505\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1878\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.9519\n",
      "1/1 [==============================] - 0s 988us/step - loss: 2.3572\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5404\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1238\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8797\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3720\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0130\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2449\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.4616\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.2405\n"
     ]
    }
   ],
   "source": [
    "# n steps forward\n",
    "# without communication\n",
    "# without dominate - subordinate relation\n",
    "n = 3\n",
    "counter = 0;\n",
    "for window in shuffle_dataset: #(18,20,31)\n",
    "    initial = window\n",
    "    \n",
    "    for i in range(n): # n step forward\n",
    "\n",
    "        output_A_0_1_ = sub_model_0_1_.predict(initial) #(18,30)\n",
    "        out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "        for i in range(batch_size):\n",
    "            pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "            one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "            out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "        output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "        y_pred_B_0_1_  #(18,30)\n",
    "        out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "        for i in range(batch_size):\n",
    "            pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "            one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "            out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "        output_A_1_0_ = sub_model_1_0_.predict(initial) #(18,30)\n",
    "        out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "        for i in range(batch_size):\n",
    "            pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "            one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "            out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "        output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "        y_pred_B_1_0_  #(18,30)\n",
    "        out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "        for i in range(batch_size):\n",
    "            pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "            one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "            out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "        target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "        target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "        target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "        target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "\n",
    "        if counter%(n*40) == 0:\n",
    "            sub_model_0_1_.fit(initial,target_1_0_0,verbose = 1)\n",
    "            sub_model_1_0_.fit(initial,target_0_1_1,verbose = 1)\n",
    "        else:\n",
    "            sub_model_0_1_.fit(initial,target_1_0_0,verbose = 0)\n",
    "            sub_model_1_0_.fit(initial,target_0_1_1,verbose = 0)\n",
    "            \n",
    "        counter = counter + 1\n",
    "        initial = tf.concat((initial[:,:-2,:],out_1_0_0,out_0_1_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 344201 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E92212BEE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0_1 - loss:  0.2652583754900843\n",
      "model_1_0 - loss:  0.3246574815874919\n",
      "model_0_1 - loss:  0.006557741165626794\n",
      "model_1_0 - loss:  0.006984730981756002\n",
      "model_0_1 - loss:  0.0036032973157707603\n",
      "model_1_0 - loss:  0.003756456454982981\n",
      "model_0_1 - loss:  0.0024886268044356256\n",
      "model_1_0 - loss:  0.0025877053467556832\n",
      "model_0_1 - loss:  0.0018996830940013751\n",
      "model_1_0 - loss:  0.0019765501485671847\n",
      "model_0_1 - loss:  0.0015337823668960483\n",
      "model_1_0 - loss:  0.0015962444654433058\n",
      "model_0_1 - loss:  0.0012892459457507356\n",
      "model_1_0 - loss:  0.0013434799746610223\n",
      "model_0_1 - loss:  0.0011068339052144438\n",
      "model_1_0 - loss:  0.0011554455670993776\n",
      "model_0_1 - loss:  0.0009724555585416965\n",
      "model_1_0 - loss:  0.0010175566663965584\n",
      "model_0_1 - loss:  0.0008679081986192614\n",
      "model_1_0 - loss:  0.0009080864900024608\n",
      "model_0_1 - loss:  0.0007805454772314988\n",
      "model_1_0 - loss:  0.0008171391857322305\n",
      "model_0_1 - loss:  0.0007118273568921722\n",
      "model_1_0 - loss:  0.0007459093842771835\n",
      "model_0_1 - loss:  0.0006522244808147661\n",
      "model_1_0 - loss:  0.0006850460370769724\n",
      "model_0_1 - loss:  0.000602181428053882\n",
      "model_1_0 - loss:  0.0006327128729317337\n",
      "model_0_1 - loss:  0.0005597630142583511\n",
      "model_1_0 - loss:  0.0005889460407197476\n",
      "model_0_1 - loss:  0.0005233473954140208\n",
      "model_1_0 - loss:  0.0005505263803643175\n",
      "model_0_1 - loss:  0.0004904439070669469\n",
      "model_1_0 - loss:  0.0005164014287583995\n",
      "model_0_1 - loss:  0.0004610233660787344\n",
      "model_1_0 - loss:  0.00048576894027064555\n",
      "model_0_1 - loss:  0.0004358903028187342\n",
      "model_1_0 - loss:  0.0004594308439118322\n",
      "model_0_1 - loss:  0.0004132483313733246\n",
      "model_1_0 - loss:  0.00043558315487462096\n",
      "model_0_1 - loss:  0.0003928664092090912\n",
      "model_1_0 - loss:  0.00041409918534918687\n",
      "model_0_1 - loss:  0.0003733955107163638\n",
      "model_1_0 - loss:  0.0003947889822884463\n",
      "model_0_1 - loss:  0.0003569651530997362\n",
      "model_1_0 - loss:  0.00037730631421436557\n",
      "model_0_1 - loss:  0.0003409051591006573\n",
      "model_1_0 - loss:  0.00036011500877793876\n",
      "model_0_1 - loss:  0.0003271082234859932\n",
      "model_1_0 - loss:  0.00034580943096079866\n",
      "model_0_1 - loss:  0.0003137845475866925\n",
      "model_1_0 - loss:  0.00033155253072618505\n",
      "model_0_1 - loss:  0.0003032902289414778\n",
      "model_1_0 - loss:  0.0003207549264188856\n",
      "model_0_1 - loss:  0.000289757691527484\n",
      "model_1_0 - loss:  0.000306819993944373\n",
      "model_0_1 - loss:  0.00028032809461001306\n",
      "model_1_0 - loss:  0.0002965901053394191\n",
      "model_0_1 - loss:  0.00027064571838127447\n",
      "model_1_0 - loss:  0.0002868149747082498\n",
      "model_0_1 - loss:  0.0002615486966678873\n",
      "model_1_0 - loss:  0.00027736155613092707\n",
      "model_0_1 - loss:  0.0002533676107268548\n",
      "model_1_0 - loss:  0.0002680640230246354\n",
      "model_0_1 - loss:  0.00024530189495999366\n",
      "model_1_0 - loss:  0.0002598840278806165\n",
      "model_0_1 - loss:  0.00023805812290811446\n",
      "model_1_0 - loss:  0.0002521905614703428\n",
      "model_0_1 - loss:  0.0002303641090838937\n",
      "model_1_0 - loss:  0.00024437194626079873\n",
      "model_0_1 - loss:  0.00022402002189483028\n",
      "model_1_0 - loss:  0.0002371824935398763\n",
      "model_0_1 - loss:  0.0002178721919481177\n",
      "model_1_0 - loss:  0.00023149645506055093\n",
      "model_0_1 - loss:  0.00021193017440964467\n",
      "model_1_0 - loss:  0.00022483124218706507\n",
      "model_0_1 - loss:  0.00020653234972269275\n",
      "model_1_0 - loss:  0.00021934194306959397\n",
      "model_0_1 - loss:  0.00020024381017719862\n",
      "model_1_0 - loss:  0.00021224813992739655\n",
      "model_0_1 - loss:  0.0001962007882830221\n",
      "model_1_0 - loss:  0.0002085245124908397\n",
      "model_0_1 - loss:  0.00019191724898701067\n",
      "model_1_0 - loss:  0.00020358155439316762\n",
      "model_0_1 - loss:  0.0001869283471314702\n",
      "model_1_0 - loss:  0.00019839734220295212\n",
      "model_0_1 - loss:  0.0001822928120236611\n",
      "model_1_0 - loss:  0.00019373945926781744\n",
      "model_0_1 - loss:  0.00017759447408025153\n",
      "model_1_0 - loss:  0.0001885666222369764\n",
      "model_0_1 - loss:  0.00017459349652926902\n",
      "model_1_0 - loss:  0.0001857285778678488\n",
      "model_0_1 - loss:  0.0001703572521364549\n",
      "model_1_0 - loss:  0.00018092633958440273\n",
      "model_0_1 - loss:  0.000166878816584358\n",
      "model_1_0 - loss:  0.00017733159431372768\n",
      "model_0_1 - loss:  0.00016329187012161127\n",
      "model_1_0 - loss:  0.00017363714434031862\n",
      "model_0_1 - loss:  0.0001599573421117384\n",
      "model_1_0 - loss:  0.00016983736428664997\n",
      "model_0_1 - loss:  0.00015688312528072858\n",
      "model_1_0 - loss:  0.00016647650829690975\n",
      "model_0_1 - loss:  0.00015399746701586991\n",
      "model_1_0 - loss:  0.00016377707387437113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-5139810170d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0moutput_A_0_1_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_model_0_1_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(18,30)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0mout_0_1_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(18,1,31)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \"\"\"\n\u001b[0;32m   1694\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4050\u001b[0m         \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4051\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4052\u001b[1;33m         **self._flat_structure)\n\u001b[0m\u001b[0;32m   4053\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMapDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, name)\u001b[0m\n\u001b[0;32m   3059\u001b[0m         \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3060\u001b[0m         \u001b[1;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3061\u001b[1;33m         \"preserve_cardinality\", preserve_cardinality)\n\u001b[0m\u001b[0;32m   3062\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# version 1\n",
    "# 1 - n steps forward\n",
    "# without communication\n",
    "# without dominate - subordinate relation\n",
    "\n",
    "step = 15\n",
    "training_times = 3;\n",
    "counter = 0;\n",
    "loss_0_1_ = 0\n",
    "loss_1_0_ = 0\n",
    "for n in range(1,step+1):\n",
    "    for times in range(training_times):\n",
    "        for window in shuffle_dataset: #(18,20,31)\n",
    "            initial = window\n",
    "            \n",
    "            for i in range(n): # n step forward\n",
    "                counter = counter + 1\n",
    "\n",
    "                output_A_0_1_ = sub_model_0_1_.predict(initial) #(18,30)\n",
    "                out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "                    pred_A_class_max_0_1_ = np.where(output_A_0_1_[i,:] == np.max(output_A_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "                y_pred_B_0_1_  #(18,30)\n",
    "                out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "                    pred_B_class_max_0_1_ = np.where(y_pred_B_0_1_[i,:] == np.max(y_pred_B_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_A_1_0_ = sub_model_1_0_.predict(initial) #(18,30)\n",
    "                out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "                    pred_A_class_max_1_0_ = np.where(output_A_1_0_[i,:] == np.max(output_A_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "                y_pred_B_1_0_  #(18,30)\n",
    "                out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "                    pred_B_class_max_1_0_ = np.where(y_pred_B_1_0_[i,:] == np.max(y_pred_B_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "                target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "\n",
    "                \n",
    "                History_0_1_ = sub_model_0_1_.fit(initial,target_1_0_0,verbose = 0)\n",
    "                loss_0_1_ = loss_0_1_ + History_0_1_.history.get('loss')[0]\n",
    "\n",
    "                History_1_0_ = sub_model_1_0_.fit(initial,target_0_1_1,verbose = 0)\n",
    "                loss_1_0_ = loss_1_0_ + History_1_0_.history.get('loss')[0]\n",
    "                \n",
    "                if counter%1000 == 0:\n",
    "                    \n",
    "                    print('model_0_1 - loss: ', loss_0_1_/1000)\n",
    "                    print('model_1_0 - loss: ', loss_1_0_/1000)\n",
    "                    loss_0_1_ = 0\n",
    "                    loss_1_0_ = 0\n",
    "\n",
    "                initial = tf.concat((initial[:,:-2,:],out_1_0_0,out_0_1_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6011555194854736"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "History.history.get('loss')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 347825 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E975714318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0_1 - loss:  0.20620975234825165\n",
      "model_1_0 - loss:  0.22923851236514747\n",
      "model_0_1 - loss:  0.01070334950229153\n",
      "model_1_0 - loss:  0.011377248265780508\n",
      "model_0_1 - loss:  0.0062285731215961275\n",
      "model_1_0 - loss:  0.0064736071899533275\n",
      "model_0_1 - loss:  0.004412614834029227\n",
      "model_1_0 - loss:  0.004532194695202634\n",
      "model_0_1 - loss:  0.003423396444413811\n",
      "model_1_0 - loss:  0.0034866099236533048\n",
      "model_0_1 - loss:  0.0027973438538610937\n",
      "model_1_0 - loss:  0.002828990068985149\n",
      "model_0_1 - loss:  0.002366253442829475\n",
      "model_1_0 - loss:  0.0023807444309350105\n",
      "model_0_1 - loss:  0.0020480897487141192\n",
      "model_1_0 - loss:  0.0020524442527676003\n",
      "model_0_1 - loss:  0.0018081698580645026\n",
      "model_1_0 - loss:  0.0018049650323810056\n",
      "model_0_1 - loss:  0.0016171624665148557\n",
      "model_1_0 - loss:  0.0016097894302802161\n",
      "model_0_1 - loss:  0.0014629098530858755\n",
      "model_1_0 - loss:  0.0014516551727429031\n",
      "model_0_1 - loss:  0.0013364156092284247\n",
      "model_1_0 - loss:  0.0013231156929396093\n",
      "model_0_1 - loss:  0.0012296888263663276\n",
      "model_1_0 - loss:  0.001213974523358047\n",
      "model_0_1 - loss:  0.001139244091231376\n",
      "model_1_0 - loss:  0.0011228704907698557\n",
      "model_0_1 - loss:  0.001059196137357503\n",
      "model_1_0 - loss:  0.0010425782646052539\n",
      "model_0_1 - loss:  0.0009917227802216076\n",
      "model_1_0 - loss:  0.0009741715828422457\n",
      "model_0_1 - loss:  0.0009305459261522628\n",
      "model_1_0 - loss:  0.0009125559416715987\n",
      "model_0_1 - loss:  0.0008791117037180811\n",
      "model_1_0 - loss:  0.0008608205010532401\n",
      "model_0_1 - loss:  0.0008285303277662024\n",
      "model_1_0 - loss:  0.0008105676934937946\n",
      "model_0_1 - loss:  0.0007890310196089558\n",
      "model_1_0 - loss:  0.0007707138803671114\n",
      "model_0_1 - loss:  0.0007490359615185298\n",
      "model_1_0 - loss:  0.0007313382684369571\n",
      "model_0_1 - loss:  0.0007145211400929838\n",
      "model_1_0 - loss:  0.000695981384254992\n",
      "model_0_1 - loss:  0.0006841208689729683\n",
      "model_1_0 - loss:  0.0006656847815029323\n",
      "model_0_1 - loss:  0.0006514879320166074\n",
      "model_1_0 - loss:  0.000634201425127685\n",
      "model_0_1 - loss:  0.0006262840996496379\n",
      "model_1_0 - loss:  0.0006087570295785553\n",
      "model_0_1 - loss:  0.0006032937669078819\n",
      "model_1_0 - loss:  0.0005858230332378298\n",
      "model_0_1 - loss:  0.0005783778696786613\n",
      "model_1_0 - loss:  0.0005611971967155114\n",
      "model_0_1 - loss:  0.0005596121484413743\n",
      "model_1_0 - loss:  0.0005421380883781239\n",
      "model_0_1 - loss:  0.0005399770583026111\n",
      "model_1_0 - loss:  0.0005227918591699563\n",
      "model_0_1 - loss:  0.0005196729044546374\n",
      "model_1_0 - loss:  0.0005031401083106175\n",
      "model_0_1 - loss:  0.0005031840155133978\n",
      "model_1_0 - loss:  0.00048680265800794587\n",
      "model_0_1 - loss:  0.0004877866277820431\n",
      "model_1_0 - loss:  0.00047135565284406767\n",
      "model_0_1 - loss:  0.0004718295422499068\n",
      "model_1_0 - loss:  0.0004560568670276552\n",
      "model_0_1 - loss:  0.00045773738986463287\n",
      "model_1_0 - loss:  0.0004419612326601055\n",
      "model_0_1 - loss:  0.000445346401509596\n",
      "model_1_0 - loss:  0.00042949806072283534\n",
      "model_0_1 - loss:  0.0004338139129104093\n",
      "model_1_0 - loss:  0.0004179016659327317\n",
      "model_0_1 - loss:  0.0004196073917846661\n",
      "model_1_0 - loss:  0.0004045155194762628\n",
      "model_0_1 - loss:  0.00040877080149948595\n",
      "model_1_0 - loss:  0.0003941204513248522\n",
      "model_0_1 - loss:  0.0003990339229640085\n",
      "model_1_0 - loss:  0.00038392918452154846\n",
      "model_0_1 - loss:  0.00038790263939881696\n",
      "model_1_0 - loss:  0.0003735141548968386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-8c9299b8a7f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mloss_0_1_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_0_1_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mHistory_0_1_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mHistory_1_0_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maux_model_1_0_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_0_1_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_0_1_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_0_1_0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[0mloss_1_0_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_1_0_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mHistory_1_0_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1725\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m     \"\"\"\n\u001b[1;32m-> 1727\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m   def interleave(self,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4121\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4122\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[1;32m-> 4123\u001b[1;33m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[0;32m   4124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4125\u001b[0m       raise TypeError(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[0;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 2939\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2941\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3363\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3364\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    347\u001b[0m       \u001b[0mfirst_k_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_in_full_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m       first_k_indices = array_ops.reshape(\n\u001b[1;32m--> 349\u001b[1;33m           first_k_indices, [num_full_batches, batch_size])\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m       \u001b[0mflat_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_k_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m   \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8232\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8233\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m-> 8234\u001b[1;33m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[0;32m   8235\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8236\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3485\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3487\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[1;32m-> 1975\u001b[1;33m                                 control_input_ops, op_def)\n\u001b[0m\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# version 2\n",
    "# 1 - n steps forward\n",
    "# with communication\n",
    "# without dominate - subordinate relation\n",
    "step = 15\n",
    "training_times = 3;\n",
    "counter = 0;\n",
    "loss_0_1_ = 0\n",
    "loss_1_0_ = 0\n",
    "for n in range(1,step+1):\n",
    "    for times in range(training_times):\n",
    "        for window in shuffle_dataset: #(18,20,31)\n",
    "            initial = window\n",
    "\n",
    "            for i in range(n): # n step forward\n",
    "                counter = counter + 1\n",
    "\n",
    "                output_A_0_1_ = sub_model_0_1_.predict(initial) #(18,30)\n",
    "                out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "                    pred_A_class_max_0_1_ = np.where(output_A_0_1_[i,:] == np.max(output_A_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "                y_pred_B_0_1_  #(18,30)\n",
    "                out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "                    pred_B_class_max_0_1_ = np.where(y_pred_B_0_1_[i,:] == np.max(y_pred_B_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_A_1_0_ = sub_model_1_0_.predict(initial) #(18,30)\n",
    "                out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "                    pred_A_class_max_1_0_ = np.where(output_A_1_0_[i,:] == np.max(output_A_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "                y_pred_B_1_0_  #(18,30)\n",
    "                out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "                    pred_B_class_max_1_0_ = np.where(y_pred_B_1_0_[i,:] == np.max(y_pred_B_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "\n",
    "                target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "                target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "\n",
    "                History_0_1_ = aux_model_0_1_.fit([initial,out_1_0_0],[target_1_0_0,target_1_0_1],verbose = 0)\n",
    "                loss_0_1_ = loss_0_1_ + History_0_1_.history.get('loss')[0]\n",
    "\n",
    "                History_1_0_ = aux_model_1_0_.fit([initial,out_0_1_1],[target_0_1_1,target_0_1_0],verbose = 0)\n",
    "                loss_1_0_ = loss_1_0_ + History_1_0_.history.get('loss')[0]\n",
    "                \n",
    "                if counter%1000 == 0:\n",
    "                    \n",
    "                    print('model_0_1 - loss: ', loss_0_1_/1000)\n",
    "                    print('model_1_0 - loss: ', loss_1_0_/1000)\n",
    "                    loss_0_1_ = 0\n",
    "                    loss_1_0_ = 0\n",
    "\n",
    "                \n",
    "                initial = tf.concat((initial[:,:-2,:],out_1_0_0,out_0_1_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 161601 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E97296C048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0_1 - loss:  1.9853931000232696\n",
      "model_0_1 - loss:  1.763319775223732\n",
      "model_0_1 - loss:  1.6686825339794158\n",
      "model_0_1 - loss:  1.5990980682969094\n",
      "model_0_1 - loss:  1.5432597593069077\n",
      "model_0_1 - loss:  1.5040169832110406\n",
      "model_0_1 - loss:  1.4893821044564246\n",
      "model_0_1 - loss:  1.4353866910338402\n",
      "model_0_1 - loss:  1.3946545179486274\n",
      "model_0_1 - loss:  1.3794402884840966\n",
      "model_0_1 - loss:  1.3635864284634591\n",
      "model_0_1 - loss:  1.3378718311190605\n",
      "model_0_1 - loss:  1.296607871234417\n",
      "model_0_1 - loss:  1.315218718290329\n",
      "model_0_1 - loss:  1.2583481413125992\n",
      "model_0_1 - loss:  1.227064264535904\n",
      "model_0_1 - loss:  1.2437001282572746\n",
      "model_0_1 - loss:  1.2070959852337837\n",
      "model_0_1 - loss:  1.19810886323452\n",
      "model_0_1 - loss:  1.1733711369037627\n",
      "model_0_1 - loss:  1.1736106811761855\n",
      "model_0_1 - loss:  1.1214050613045692\n",
      "model_0_1 - loss:  1.1285535547733307\n",
      "model_0_1 - loss:  1.112572342157364\n",
      "model_0_1 - loss:  1.1029101669788361\n",
      "model_0_1 - loss:  1.1022457077503205\n",
      "model_0_1 - loss:  1.0779580165743827\n",
      "model_0_1 - loss:  1.0718438214063644\n",
      "model_0_1 - loss:  1.0601379282474517\n",
      "model_0_1 - loss:  1.0371124895215034\n",
      "model_0_1 - loss:  1.0295188215970994\n",
      "model_0_1 - loss:  1.0165474033355713\n",
      "model_0_1 - loss:  1.023499158978462\n",
      "model_0_1 - loss:  0.9961433216929436\n",
      "model_0_1 - loss:  0.9811364293694497\n",
      "model_0_1 - loss:  0.9812121357917786\n",
      "model_0_1 - loss:  0.9840635871589184\n",
      "model_0_1 - loss:  0.9720369152128696\n",
      "model_0_1 - loss:  0.9791037859022618\n",
      "model_0_1 - loss:  0.9123438783586025\n",
      "model_0_1 - loss:  0.941394313633442\n",
      "model_0_1 - loss:  0.9588673114776611\n",
      "model_0_1 - loss:  0.9213526914715767\n",
      "model_0_1 - loss:  0.9171310000419617\n",
      "model_0_1 - loss:  0.9071772409975529\n",
      "model_0_1 - loss:  0.8877526983618736\n",
      "model_0_1 - loss:  0.8986341756284237\n",
      "model_0_1 - loss:  0.8817050779163838\n",
      "model_0_1 - loss:  0.8760945009887219\n",
      "model_0_1 - loss:  0.8579136394262313\n",
      "model_0_1 - loss:  0.8832393473088741\n",
      "model_0_1 - loss:  0.8505245098471641\n",
      "model_0_1 - loss:  0.863040101736784\n",
      "model_0_1 - loss:  0.8731471362709999\n",
      "model_0_1 - loss:  0.83242404961586\n",
      "model_0_1 - loss:  0.8417675563693047\n",
      "model_0_1 - loss:  0.8003646193742752\n",
      "model_0_1 - loss:  0.8240225076675415\n",
      "model_0_1 - loss:  0.8343551715016365\n",
      "model_0_1 - loss:  0.8118665571808815\n",
      "model_0_1 - loss:  0.7926435613036156\n",
      "model_0_1 - loss:  0.8125618849396705\n",
      "model_0_1 - loss:  0.790171667277813\n",
      "model_0_1 - loss:  0.7953923591971397\n",
      "model_0_1 - loss:  0.792874656200409\n",
      "model_0_1 - loss:  0.7694228344261647\n",
      "model_0_1 - loss:  0.7905378478169441\n",
      "model_0_1 - loss:  0.759343205422163\n",
      "model_0_1 - loss:  0.7642246123254299\n",
      "model_0_1 - loss:  0.7344006250500679\n",
      "model_0_1 - loss:  0.7518246977627278\n",
      "model_0_1 - loss:  0.771084607809782\n",
      "model_0_1 - loss:  0.7591461662948131\n",
      "model_0_1 - loss:  0.7260962852984667\n",
      "model_0_1 - loss:  0.7312838944792748\n",
      "model_0_1 - loss:  0.7372788617610931\n",
      "model_0_1 - loss:  0.7277658139169216\n",
      "model_0_1 - loss:  0.7010946357548237\n",
      "model_0_1 - loss:  0.7047228453159332\n",
      "model_0_1 - loss:  0.6961212938129901\n",
      "model_0_1 - loss:  0.7497010089755058\n",
      "model_0_1 - loss:  0.6864247836321592\n",
      "model_0_1 - loss:  0.6871088847219944\n",
      "model_0_1 - loss:  0.7112482588291168\n",
      "model_0_1 - loss:  0.6798610716313124\n",
      "model_0_1 - loss:  0.7175079105049371\n"
     ]
    }
   ],
   "source": [
    "# version 3\n",
    "# 1 - n steps forward\n",
    "# without communication\n",
    "# with single side dominate - subordinate relation\n",
    "\n",
    "# same as:\n",
    "# adapt my mind to the observation\n",
    "# no action applied\n",
    "\n",
    "step = 15\n",
    "training_times = 3;\n",
    "counter = 0;\n",
    "loss_0_1_ = 0\n",
    "\n",
    "for n in range(1,step+1):\n",
    "    for times in range(training_times):\n",
    "        for window in shuffle_dataset: #(18,20,31)\n",
    "            initial = window\n",
    "\n",
    "            for i in range(n): # n step forward\n",
    "                counter = counter + 1\n",
    "\n",
    "                output_A_0_1_ = sub_model_0_1_.predict(initial) #(18,30)\n",
    "                out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "                    pred_A_class_max_0_1_ = np.where(output_A_0_1_[i,:] == np.max(output_A_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "                y_pred_B_0_1_  #(18,30)\n",
    "                out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "                    pred_B_class_max_0_1_ = np.where(y_pred_B_0_1_[i,:] == np.max(y_pred_B_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_A_1_0_ = sub_model_1_0_.predict(initial) #(18,30)\n",
    "                out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "                    pred_A_class_max_1_0_ = np.where(output_A_1_0_[i,:] == np.max(output_A_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "                y_pred_B_1_0_  #(18,30)\n",
    "                out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "                    pred_B_class_max_1_0_ = np.where(y_pred_B_1_0_[i,:] == np.max(y_pred_B_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "\n",
    "                target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "                target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "\n",
    "                History_0_1_ = sub_model_0_1_.fit(initial,target_1_0_0,verbose = 0)\n",
    "                loss_0_1_ = loss_0_1_ + History_0_1_.history.get('loss')[0]\n",
    "                \n",
    "                if counter%1000 == 0:\n",
    "                    print('model_0_1 - loss: ', loss_0_1_/1000)\n",
    "                    loss_0_1_ = 0\n",
    "                    \n",
    "                initial = tf.concat((initial[:,:-2,:],out_1_0_0,out_0_1_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 208813 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E93670BA68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0_1 - loss:  0.43955828779470174\n",
      "model_1_0 - loss:  0.4716214130232111\n",
      "model_0_1 - loss:  0.007909399166703223\n",
      "model_1_0 - loss:  0.007167036981787532\n",
      "model_0_1 - loss:  0.003696445174748078\n",
      "model_1_0 - loss:  0.003912661943119019\n",
      "model_0_1 - loss:  0.0025852611139416696\n",
      "model_1_0 - loss:  0.0025333449493627996\n",
      "model_0_1 - loss:  0.0019399746045237407\n",
      "model_1_0 - loss:  0.0019240923252655192\n",
      "model_0_1 - loss:  0.0015254788940073922\n",
      "model_1_0 - loss:  0.0015837491110432893\n",
      "model_0_1 - loss:  0.0012355579438153655\n",
      "model_1_0 - loss:  0.0013610715855611488\n",
      "model_0_1 - loss:  0.001060215443954803\n",
      "model_1_0 - loss:  0.0011669691242277623\n",
      "model_0_1 - loss:  0.000959481451427564\n",
      "model_1_0 - loss:  0.0009942673149635085\n",
      "model_0_1 - loss:  0.000831947589002084\n",
      "model_1_0 - loss:  0.0009055787406978198\n",
      "model_0_1 - loss:  0.0007486671467195265\n",
      "model_1_0 - loss:  0.0008168068425729871\n",
      "model_0_1 - loss:  0.0007050269345054402\n",
      "model_1_0 - loss:  0.0007186501563992352\n",
      "model_0_1 - loss:  0.0006484981995308771\n",
      "model_1_0 - loss:  0.0006568505632458255\n",
      "model_0_1 - loss:  0.0005964084577281028\n",
      "model_1_0 - loss:  0.000609126235358417\n",
      "model_0_1 - loss:  0.0005606129886000417\n",
      "model_1_0 - loss:  0.0005588204592932016\n",
      "model_0_1 - loss:  0.0005123178970243316\n",
      "model_1_0 - loss:  0.0005315299875801429\n",
      "model_0_1 - loss:  0.0004728383393958211\n",
      "model_1_0 - loss:  0.0005062984339310788\n",
      "model_0_1 - loss:  0.0004526674596127123\n",
      "model_1_0 - loss:  0.0004702317559567746\n",
      "model_0_1 - loss:  0.00043937080158502797\n",
      "model_1_0 - loss:  0.0004323258620861452\n",
      "model_0_1 - loss:  0.00041533840203192084\n",
      "model_1_0 - loss:  0.0004098567173350602\n",
      "model_0_1 - loss:  0.0003920000536018051\n",
      "model_1_0 - loss:  0.0003913313866360113\n",
      "model_0_1 - loss:  0.00037001965497620405\n",
      "model_1_0 - loss:  0.00037618859022040853\n",
      "model_0_1 - loss:  0.0003493394051911309\n",
      "model_1_0 - loss:  0.00036456594816991126\n",
      "model_0_1 - loss:  0.00035463450578390623\n",
      "model_1_0 - loss:  0.0003249154310324229\n",
      "model_0_1 - loss:  0.00032219226288725624\n",
      "model_1_0 - loss:  0.000332043859641999\n",
      "model_0_1 - loss:  0.00031449648123816585\n",
      "model_1_0 - loss:  0.0003120636525854934\n",
      "model_0_1 - loss:  0.0002956945575715508\n",
      "model_1_0 - loss:  0.0003056792641000357\n",
      "model_0_1 - loss:  0.00028698782515130004\n",
      "model_1_0 - loss:  0.0002951127158594318\n",
      "model_0_1 - loss:  0.0002661654605763033\n",
      "model_1_0 - loss:  0.00029417041875422\n",
      "model_0_1 - loss:  0.000264509275497403\n",
      "model_1_0 - loss:  0.0002755435600702185\n",
      "model_0_1 - loss:  0.0002552622540097218\n",
      "model_1_0 - loss:  0.0002668886097962968\n",
      "model_0_1 - loss:  0.0002512634536251426\n",
      "model_1_0 - loss:  0.0002530780131928623\n",
      "model_0_1 - loss:  0.0002452196337399073\n",
      "model_1_0 - loss:  0.00024472099512058774\n",
      "model_0_1 - loss:  0.00024117791349999608\n",
      "model_1_0 - loss:  0.00023375793659943155\n",
      "model_0_1 - loss:  0.0002324222792085493\n",
      "model_1_0 - loss:  0.0002280060206539929\n",
      "model_0_1 - loss:  0.0002165443261474138\n",
      "model_1_0 - loss:  0.0002315309462719597\n",
      "model_0_1 - loss:  0.00021379002364119515\n",
      "model_1_0 - loss:  0.00022154615598265082\n",
      "model_0_1 - loss:  0.00021413638634840026\n",
      "model_1_0 - loss:  0.00020903558691497893\n",
      "model_0_1 - loss:  0.00020182716315321158\n",
      "model_1_0 - loss:  0.00021061498422932345\n",
      "model_0_1 - loss:  0.0002030039778619539\n",
      "model_1_0 - loss:  0.00019837930050562136\n",
      "model_0_1 - loss:  0.000196741438441677\n",
      "model_1_0 - loss:  0.00019408698147162795\n",
      "model_0_1 - loss:  0.00019195726941688917\n",
      "model_1_0 - loss:  0.00019049464992713184\n",
      "model_0_1 - loss:  0.00019188330676115584\n",
      "model_1_0 - loss:  0.00018050406393012964\n"
     ]
    }
   ],
   "source": [
    "# version 4\n",
    "# 1 - n steps forward\n",
    "# without communication\n",
    "# with either side dominate - subordinate relation\n",
    "step = 15\n",
    "training_times = 3;\n",
    "counter = 0;\n",
    "loss_0_1_ = 0\n",
    "loss_1_0_ = 0\n",
    "for n in range(1,step+1):\n",
    "    for times in range(training_times):\n",
    "        for window in shuffle_dataset: #(18,20,31)\n",
    "            initial = window\n",
    "\n",
    "            for i in range(n): # n step forward\n",
    "                counter = counter + 1\n",
    "\n",
    "                output_A_0_1_ = sub_model_0_1_.predict(initial) #(18,30)\n",
    "                out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "                    pred_A_class_max_0_1_ = np.where(output_A_0_1_[i,:] == np.max(output_A_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "                y_pred_B_0_1_  #(18,30)\n",
    "                out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "                    pred_B_class_max_0_1_ = np.where(y_pred_B_0_1_[i,:] == np.max(y_pred_B_0_1_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_0_1_, 30),[1,1,30])\n",
    "\n",
    "                    out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_A_1_0_ = sub_model_1_0_.predict(initial) #(18,30)\n",
    "                out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "                    pred_A_class_max_1_0_ = np.where(output_A_1_0_[i,:] == np.max(output_A_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_A_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "                y_pred_B_1_0_  #(18,30)\n",
    "                out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                for i in range(batch_size):\n",
    "                    # pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "                    pred_B_class_max_1_0_ = np.where(y_pred_B_1_0_[i,:] == np.max(y_pred_B_1_0_[i,:]))[0][0]\n",
    "\n",
    "                    # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "                    one_hot = tf.reshape(tf.one_hot(pred_B_class_max_1_0_, 30),[1,1,30])\n",
    "\n",
    "                    out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "\n",
    "                target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "                target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "                target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "\n",
    "                if np.random.rand(1)[0] < 0.5:\n",
    "                    History_0_1_ = sub_model_0_1_.fit(initial,target_1_0_0,verbose = 0) \n",
    "                    loss_0_1_ = loss_0_1_ + History_0_1_.history.get('loss')[0]\n",
    "                else:\n",
    "                    History_1_0_ = sub_model_1_0_.fit(initial,target_0_1_1,verbose = 0)\n",
    "                    loss_1_0_ = loss_1_0_ + History_1_0_.history.get('loss')[0]\n",
    "\n",
    "                if counter%2000 == 0:\n",
    "                    \n",
    "                    print('model_0_1 - loss: ', loss_0_1_/1000)\n",
    "                    print('model_1_0 - loss: ', loss_1_0_/1000)\n",
    "                    loss_0_1_ = 0\n",
    "                    loss_1_0_ = 0\n",
    "                    \n",
    "                initial = tf.concat((initial[:,:-2,:],out_1_0_0,out_0_1_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0_1 - loss:  2.734954721689224\n",
      "model_0_1 - loss:  2.2768907078504563\n",
      "model_0_1 - loss:  2.07823702788353\n",
      "model_0_1 - loss:  1.9759793725013732\n",
      "model_0_1 - loss:  1.8553427128791808\n",
      "model_0_1 - loss:  1.7616099138855934\n",
      "model_0_1 - loss:  1.7434243019223212\n",
      "model_0_1 - loss:  1.6472704045772553\n",
      "model_0_1 - loss:  1.5884761747717857\n",
      "model_0_1 - loss:  1.6012948861718177\n",
      "model_0_1 - loss:  1.5375391768217086\n",
      "model_0_1 - loss:  1.4962132036685944\n",
      "model_0_1 - loss:  1.459306826353073\n",
      "model_0_1 - loss:  1.4436699231266976\n",
      "model_0_1 - loss:  1.4214068812131881\n",
      "model_0_1 - loss:  1.38073533898592\n",
      "model_0_1 - loss:  1.389890702188015\n",
      "model_0_1 - loss:  1.3308183543086052\n",
      "model_0_1 - loss:  1.3199413604140282\n",
      "model_0_1 - loss:  1.3131540709733962\n",
      "model_0_1 - loss:  1.286356808781624\n",
      "model_0_1 - loss:  1.2619687846899033\n",
      "model_0_1 - loss:  1.2399959751367569\n",
      "model_0_1 - loss:  1.244893409192562\n",
      "model_0_1 - loss:  1.2210461108088493\n",
      "model_0_1 - loss:  1.247340129494667\n",
      "model_0_1 - loss:  1.147494936466217\n",
      "model_0_1 - loss:  1.1781732884645462\n",
      "model_0_1 - loss:  1.1502426600456237\n",
      "model_0_1 - loss:  1.1543907914459706\n",
      "model_0_1 - loss:  1.1246486193239689\n",
      "model_0_1 - loss:  1.153936596930027\n",
      "model_0_1 - loss:  1.078912277162075\n",
      "model_0_1 - loss:  1.093438131093979\n",
      "model_0_1 - loss:  1.0952387434840203\n",
      "model_0_1 - loss:  1.0673558660745621\n",
      "model_0_1 - loss:  1.0550810549259186\n",
      "model_0_1 - loss:  1.0374885281026363\n",
      "model_0_1 - loss:  1.0453993619680404\n",
      "model_0_1 - loss:  1.0044821812808513\n",
      "model_0_1 - loss:  1.0163685595393182\n",
      "model_0_1 - loss:  1.0143267927765847\n",
      "model_0_1 - loss:  0.988126494884491\n",
      "model_0_1 - loss:  1.0187266602814198\n",
      "model_0_1 - loss:  0.9701247921586037\n",
      "model_0_1 - loss:  0.9515612791478634\n",
      "model_0_1 - loss:  0.955676796734333\n",
      "model_0_1 - loss:  0.9392730203270913\n",
      "model_0_1 - loss:  0.9281221096813679\n",
      "model_0_1 - loss:  0.9282791655063629\n",
      "model_0_1 - loss:  0.9241966382265091\n",
      "model_0_1 - loss:  0.8913179686963558\n",
      "model_0_1 - loss:  0.9213816988170147\n",
      "model_0_1 - loss:  0.8796433824896812\n",
      "model_0_1 - loss:  0.9331785496473313\n",
      "model_0_1 - loss:  0.8601368091106415\n",
      "model_0_1 - loss:  0.9009730192720891\n",
      "model_0_1 - loss:  0.8567112423479557\n",
      "model_0_1 - loss:  0.845503685683012\n",
      "model_0_1 - loss:  0.838352324694395\n",
      "model_0_1 - loss:  0.8491645601391792\n",
      "model_0_1 - loss:  0.8448679039478302\n",
      "model_0_1 - loss:  0.8280578328669072\n",
      "model_0_1 - loss:  0.8120832804441452\n",
      "model_0_1 - loss:  0.8274960281252861\n",
      "model_0_1 - loss:  0.8035642388164997\n",
      "model_0_1 - loss:  0.7792910111397505\n",
      "model_0_1 - loss:  0.8315701666474342\n",
      "model_0_1 - loss:  0.8176780433356762\n",
      "model_0_1 - loss:  0.7732262354791164\n",
      "model_0_1 - loss:  0.800639809012413\n",
      "model_0_1 - loss:  0.7739432332813739\n",
      "model_0_1 - loss:  0.7784675514996052\n",
      "model_0_1 - loss:  0.7769590988904238\n",
      "model_0_1 - loss:  0.7851700083911419\n",
      "model_0_1 - loss:  0.761267321407795\n",
      "model_0_1 - loss:  0.7325495829284191\n",
      "model_0_1 - loss:  0.7503254800885916\n",
      "model_0_1 - loss:  0.7731961260735989\n",
      "model_0_1 - loss:  0.7377338484823703\n",
      "model_0_1 - loss:  0.7547389631420374\n",
      "model_0_1 - loss:  0.7369970013797283\n",
      "model_0_1 - loss:  0.7290142564475537\n",
      "model_0_1 - loss:  0.7839712781608105\n",
      "model_0_1 - loss:  0.7096686616241932\n",
      "model_0_1 - loss:  0.7223668533712625\n"
     ]
    }
   ],
   "source": [
    "# version 5\n",
    "# 1 - n steps forward\n",
    "# with communication\n",
    "# with single side dominate - subordinate relation\n",
    "\n",
    "step = 15\n",
    "training_times = 3;\n",
    "counter = 0;\n",
    "loss_0_1_ = 0;\n",
    "for epoch in range(1):\n",
    "    for n in range(1,step+1):\n",
    "        for times in range(training_times):\n",
    "            for window in shuffle_dataset: #(18,20,31)\n",
    "                initial = window\n",
    "\n",
    "                for i in range(n): # n step forward\n",
    "                    counter = counter + 1\n",
    "\n",
    "                    output_A_0_1_ = sub_model_0_1_.predict(initial) #(18,30)\n",
    "                    out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "                        pred_A_class_max_0_1_ = np.where(output_A_0_1_[i,:] == np.max(output_A_0_1_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_A_class_max_0_1_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                    output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "                    y_pred_B_0_1_  #(18,30)\n",
    "                    out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "                        pred_B_class_max_0_1_ = np.where(y_pred_B_0_1_[i,:] == np.max(y_pred_B_0_1_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_B_class_max_0_1_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                    output_A_1_0_ = sub_model_1_0_.predict(initial) #(18,30)\n",
    "                    out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "                        pred_A_class_max_1_0_ = np.where(output_A_1_0_[i,:] == np.max(output_A_1_0_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_A_class_max_1_0_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                    output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "                    y_pred_B_1_0_  #(18,30)\n",
    "                    out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "                        pred_B_class_max_1_0_ = np.where(y_pred_B_1_0_[i,:] == np.max(y_pred_B_1_0_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_B_class_max_1_0_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                    target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "                    target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "                    target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "                    target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "\n",
    "                    History_0_1_ = aux_model_0_1_.fit([initial,out_1_0_0],[target_1_0_0,target_1_0_1],verbose = 0)\n",
    "                    loss_0_1_ = loss_0_1_ + History_0_1_.history.get('loss')[0]\n",
    "\n",
    "                    if counter%1000 == 0:\n",
    "                        print('model_0_1 - loss: ', loss_0_1_/1000)\n",
    "                        loss_0_1_ = 0\n",
    "\n",
    "                    initial = tf.concat((initial[:,:-2,:],out_1_0_0,out_0_1_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0_1 - loss:  0.1747257892070338\n",
      "model_1_0 - loss:  0.18830171506665647\n",
      "model_0_1 - loss:  0.010841891036834568\n",
      "model_1_0 - loss:  0.010778524483554066\n",
      "model_0_1 - loss:  0.006334565908182412\n",
      "model_1_0 - loss:  0.006184166882652789\n",
      "model_0_1 - loss:  0.00428621018002741\n",
      "model_1_0 - loss:  0.004562880933983251\n",
      "model_0_1 - loss:  0.0034175378396175803\n",
      "model_1_0 - loss:  0.003422798012616113\n",
      "model_0_1 - loss:  0.0027646566529292613\n",
      "model_1_0 - loss:  0.002819224027916789\n",
      "model_0_1 - loss:  0.0023918168831150977\n",
      "model_1_0 - loss:  0.002316170803969726\n",
      "model_0_1 - loss:  0.0019415372186340392\n",
      "model_1_0 - loss:  0.002127920774393715\n",
      "model_0_1 - loss:  0.0017300101198488846\n",
      "model_1_0 - loss:  0.0018551852572709323\n",
      "model_0_1 - loss:  0.0016110781442839651\n",
      "model_1_0 - loss:  0.001594420215464197\n",
      "model_0_1 - loss:  0.0015399826674256474\n",
      "model_1_0 - loss:  0.0013579110694117843\n",
      "model_0_1 - loss:  0.0013480629787081852\n",
      "model_1_0 - loss:  0.001292885557981208\n",
      "model_0_1 - loss:  0.0012365215381141752\n",
      "model_1_0 - loss:  0.001192080119275488\n",
      "model_0_1 - loss:  0.0011240708158584312\n",
      "model_1_0 - loss:  0.0011206986212637276\n",
      "model_0_1 - loss:  0.0010826042243279516\n",
      "model_1_0 - loss:  0.0010071202982799151\n",
      "model_0_1 - loss:  0.001008343795372639\n",
      "model_1_0 - loss:  0.000944233562971931\n",
      "model_0_1 - loss:  0.0009444708216469735\n",
      "model_1_0 - loss:  0.0008905697824666276\n",
      "model_0_1 - loss:  0.0008701667774002999\n",
      "model_1_0 - loss:  0.0008567970294388942\n",
      "model_0_1 - loss:  0.0008392258534440771\n",
      "model_1_0 - loss:  0.0007926333912764676\n",
      "model_0_1 - loss:  0.0007796854010666721\n",
      "model_1_0 - loss:  0.0007662607286474667\n",
      "model_0_1 - loss:  0.000770873611967545\n",
      "model_1_0 - loss:  0.0007018004442797973\n",
      "model_0_1 - loss:  0.0007014284544275142\n",
      "model_1_0 - loss:  0.0006992112168809399\n",
      "model_0_1 - loss:  0.0006687725550727919\n",
      "model_1_0 - loss:  0.0006709769319277256\n",
      "model_0_1 - loss:  0.0006457921171095222\n",
      "model_1_0 - loss:  0.0006356020677485503\n",
      "model_0_1 - loss:  0.0006437560542835854\n",
      "model_1_0 - loss:  0.0005862141649704426\n",
      "model_0_1 - loss:  0.0006217021951451897\n",
      "model_1_0 - loss:  0.0005562708079232834\n",
      "model_0_1 - loss:  0.000570395509537775\n",
      "model_1_0 - loss:  0.0005649273676099256\n",
      "model_0_1 - loss:  0.0005584747059037909\n",
      "model_1_0 - loss:  0.0005354060215759091\n",
      "model_0_1 - loss:  0.0005578563169110567\n",
      "model_1_0 - loss:  0.0004959461221005768\n",
      "model_0_1 - loss:  0.0005041294721304439\n",
      "model_1_0 - loss:  0.0005150712219474372\n",
      "model_0_1 - loss:  0.0004934658735583071\n",
      "model_1_0 - loss:  0.0004895838131196797\n",
      "model_0_1 - loss:  0.000479198837565491\n",
      "model_1_0 - loss:  0.00047293441256624646\n",
      "model_0_1 - loss:  0.0004893476275901775\n",
      "model_1_0 - loss:  0.00043476637729327193\n",
      "model_0_1 - loss:  0.00045356150000588967\n",
      "model_1_0 - loss:  0.00044083662220509724\n",
      "model_0_1 - loss:  0.00042759411336737685\n",
      "model_1_0 - loss:  0.00044025559155852533\n",
      "model_0_1 - loss:  0.0004242936676600948\n",
      "model_1_0 - loss:  0.00041998115749447605\n",
      "model_0_1 - loss:  0.00043007384485099467\n",
      "model_1_0 - loss:  0.0003910461709310766\n",
      "model_0_1 - loss:  0.0003983185437391512\n",
      "model_1_0 - loss:  0.00039963746411376635\n",
      "model_0_1 - loss:  0.0004229916382173542\n",
      "model_1_0 - loss:  0.0003555507561250124\n",
      "model_0_1 - loss:  0.0003809778640861623\n",
      "model_1_0 - loss:  0.00037720514577813444\n",
      "model_0_1 - loss:  0.000371715476794634\n",
      "model_1_0 - loss:  0.000365169854456326\n",
      "model_0_1 - loss:  0.00037886575295124203\n",
      "model_1_0 - loss:  0.0003442437045450788\n",
      "model_0_1 - loss:  0.00035742633961490357\n",
      "model_1_0 - loss:  0.0003439718130102847\n"
     ]
    }
   ],
   "source": [
    "# version 6\n",
    "# 1 - n steps forward\n",
    "# with communication\n",
    "# with either side dominate - subordinate relation\n",
    "step = 15\n",
    "training_times = 3;\n",
    "counter = 0;\n",
    "loss_0_1_ = 0\n",
    "loss_1_0_ = 0\n",
    "for epoch in range(1):\n",
    "    for n in range(1,step+1):\n",
    "        for times in range(training_times):\n",
    "            for window in shuffle_dataset: #(18,20,31)\n",
    "                initial = window\n",
    "\n",
    "                for i in range(n): # n step forward\n",
    "                    counter = counter + 1\n",
    "\n",
    "                    output_A_0_1_ = sub_model_0_1_.predict(initial) #(18,30)\n",
    "                    out_0_1_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_A_class_random_0_1_ = np.random.choice(30, 1, p=output_A_0_1_[i,:])[0]\n",
    "                        pred_A_class_max_0_1_ = np.where(output_A_0_1_[i,:] == np.max(output_A_0_1_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_0_1_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_A_class_max_0_1_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_0_1_0[i:i+1,:,:] = np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                    output_RNN, y_pred_A_0_1_, y_pred_B_0_1_ = model_0_1_.predict((initial,out_0_1_0))\n",
    "                    y_pred_B_0_1_  #(18,30)\n",
    "                    out_0_1_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_B_class_random_0_1_ = np.random.choice(30, 1, p=y_pred_B_0_1_[i,:])[0]\n",
    "                        pred_B_class_max_0_1_ = np.where(y_pred_B_0_1_[i,:] == np.max(y_pred_B_0_1_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_0_1_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_B_class_max_0_1_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_0_1_1[i:i+1,:,:]= np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                    output_A_1_0_ = sub_model_1_0_.predict(initial) #(18,30)\n",
    "                    out_1_0_1 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_A_class_random_1_0_ = np.random.choice(30, 1, p=output_A_1_0_[i,:])[0]\n",
    "                        pred_A_class_max_1_0_ = np.where(output_A_1_0_[i,:] == np.max(output_A_1_0_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_A_class_random_1_0_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_A_class_max_1_0_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_1_0_1[i:i+1,:,:] = np.concatenate((one_hot,[[[1]]]),2) #(18,1,31)\n",
    "\n",
    "                    output_RNN, y_pred_A_1_0_, y_pred_B_1_0_ = model_1_0_.predict((initial,out_1_0_1))\n",
    "                    y_pred_B_1_0_  #(18,30)\n",
    "                    out_1_0_0 = np.zeros([batch_size,1,31]) #(18,1,31)\n",
    "                    for i in range(batch_size):\n",
    "                        # pred_B_class_random_1_0_ = np.random.choice(30, 1, p=y_pred_B_1_0_[i,:])[0]\n",
    "                        pred_B_class_max_1_0_ = np.where(y_pred_B_1_0_[i,:] == np.max(y_pred_B_1_0_[i,:]))[0][0]\n",
    "                        \n",
    "                        # one_hot = tf.reshape(tf.one_hot(pred_B_class_random_1_0_, 30),[1,1,30])\n",
    "                        one_hot = tf.reshape(tf.one_hot(pred_B_class_max_1_0_, 30),[1,1,30])\n",
    "                        \n",
    "                        out_1_0_0[i:i+1,:,:]= np.concatenate((one_hot,[[[0]]]),2) #(18,1,31)\n",
    "\n",
    "                    target_0_1_0 = np.reshape(out_0_1_0,[batch_size,31])[:,:-1]\n",
    "                    target_0_1_1 = np.reshape(out_0_1_1,[batch_size,31])[:,:-1]\n",
    "                    target_1_0_1 = np.reshape(out_1_0_1,[batch_size,31])[:,:-1]\n",
    "                    target_1_0_0 = np.reshape(out_1_0_0,[batch_size,31])[:,:-1]\n",
    "\n",
    "                    if np.random.rand(1)[0] < 0.5:\n",
    "                        History_0_1_ = aux_model_0_1_.fit([initial,out_1_0_0],[target_1_0_0,target_1_0_1],verbose = 0)\n",
    "                        loss_0_1_ = loss_0_1_ + History_0_1_.history.get('loss')[0]\n",
    "                    else:\n",
    "                        History_1_0_ = aux_model_1_0_.fit([initial,out_0_1_1],[target_0_1_1,target_0_1_0],verbose = 0)\n",
    "                        loss_1_0_ = loss_1_0_ + History_1_0_.history.get('loss')[0]\n",
    "\n",
    "                    if counter%2000 == 0:\n",
    "\n",
    "                        print('model_0_1 - loss: ', loss_0_1_/1000)\n",
    "                        print('model_1_0 - loss: ', loss_1_0_/1000)\n",
    "                        loss_0_1_ = 0\n",
    "                        loss_1_0_ = 0\n",
    "\n",
    "                    initial = tf.concat((initial[:,:-2,:],out_1_0_0,out_0_1_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_1_.save_weights('version4_withoutCommunication_withEitherDomSub_model_0_1_.h5')\n",
    "model_1_0_.save_weights('version4_withoutCommunication_withEitherDomSub_model_1_0_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = np.zeros((30,))\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prob = np.array([1,4,5])\n",
    "np.where(prob == np.max(prob))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1_0_1 =  tf.reshape(prob,[2,1,15])\n",
    "one = tf.reshape(tf.ones(2),[2,1,1])\n",
    "out_1_0_1 =  np.concatenate((out_1_0_1,one),2) #(18,1,31)\n",
    "out_1_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 15), dtype=float64, numpy=\n",
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
